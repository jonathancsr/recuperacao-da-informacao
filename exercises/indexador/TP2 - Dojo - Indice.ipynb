{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta pr√°tica voc√™ ir√° implementar o indexador para, logo ap√≥s, indexar o conte√∫do da Wikip√©dia. Nesta pr√°tica, o √≠ndice √© composto pela classe abstrata `Index` que armazena a estrutura do √≠ndice e possui as opera√ß√µes b√°sicas do mesmo. \n",
    "\n",
    "Iremos fazer duas implementa√ß√µes desse √≠ndice: o `HashIndex` que ser√° um √≠ndice simples em mem√≥ria principal e o `FileIndex` em que as ocorr√™ncias ficar√£o em mem√≥ria secund√°ria para possibilitar a indexa√ß√£o de uma quantidade maior de p√°ginas. Assim, teremos os seguintes arquivos:\n",
    "\n",
    "- `structure.py`: Possui toda a estrutura do √≠ndice;\n",
    "- `index_structure_test.py`: Testa a estrutura do √≠ndice;\n",
    "- `file_index_test.py`: Possui os testes unit√°rios espec√≠ficos para a indexa√ß√£o das ocorrencias em arquivos da classe `FileIndex`;\n",
    "- `performance_test.py`: Executa um teste de performance (tempo de execu√ß√£o e mem√≥ria utilizada) do √≠ndice;\n",
    "- `indexer.py`: Possui as classes para o preprocessamento e prepara√ß√£o para a indexa√ß√£o;\n",
    "- `indexer_test.py`: Realiza o [teste de integra√ß√£o](https://en.wikipedia.org/wiki/Integration_testing) da pr√°tica como um todo (inclusive as fun√ß√µes da indexer.py).\n",
    "\n",
    "Na entrega, n√£o esque√ßa de apresentar a sa√≠da de execu√ß√£o de cada atividade desta tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementa√ß√£o da classe Abstrata `Index` e classe `HashIndex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe abstrata Index, no arquivo `structure.py` possui os seguintes atributos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dic_index`:  dicion√°rio em que a chave √© o termo indexa√ß√£o (string, gerenciado por esta classe) e, os valores,  podem der de diferentes tipos - dependendo da subclasse;\n",
    "- `set_documents`: conjunto de ids de documentos existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os m√©todos ser√£o discutidos ao longo das atividades. Inicialmente, iremos fazer a importa√ß√£o do m√≥dulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.structure import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - m√©todo index da classe Index**: Este m√©todo esta quase todo pronto e √© repons√°vel por indexar um termo com sua frequncia e documento no √≠ndice, de acordo com uma de suas subclasses. Voc√™ deve deve apenas inicializar a vari√°vel `int_term_id` apropriadamente - substituindo os `None` correspondente. Caso o termo n√£o exista no √≠ndice, dever√° obter o pr√≥ximo term_id. Esse id pode ser sequencial. Caso esse id seja encontrado, a classe Index dever√° chamar o m√©todo `get_term_id` (implementado pelas subclasses) para obt√™-lo pois, dependendo da implementa√ß√£o, haver√° uma forma diferente de obten√ß√£o. Al√©m disso, voc√™ dever√° atualizar o atributo `set_documents` apropriadamente. Para testar tanto esta atividade e a seguinte,  voc√™ dever√° fazer uma das implementa√ß√µes dessa classe - a classe **HashIndex** - na atividade 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - atributos calculados da classe Index**: Voc√™ deve implementar os atributos calculados `document_count` e `vocabulary` da classe Index. O atributo `document_count` retorna a quantidade de documentos existentes e, `vocabulary` retorna uma lista com o vocabul√°rio completo indexado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - Implementa√ß√£o da classe HashIndex: ** O HashIndex dever√° fazer um √≠ndice em mem√≥ria.\n",
    "\n",
    "Como exemplo, caso tenhamos tr√™s documentos $d_1 = $\"A casa verde √© uma casa bonita\", $d_2 = $\"A casa bonita\" e $d_3 =$\"O pr√©dio verde\", caso n√£o haja remo√ß√£o de _stopwords_ nem acentos, o atributo `dic_index` dever√° possuir a seguinte estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [(term_id:1 doc: 1 freq: 1), (term_id:1 doc: 2 freq: 1)],\n",
       " 'casa': [(term_id:2 doc: 1 freq: 2), (term_id:2 doc: 2 freq: 1)],\n",
       " 'verde': [(term_id:3 doc: 1 freq: 1), (term_id:3 doc: 3 freq: 1)],\n",
       " '√©': [(term_id:4 doc: 1 freq: 1)],\n",
       " 'uma': [(term_id:5 doc: 1 freq: 1)],\n",
       " 'bonita': [(term_id:6 doc: 1 freq: 1), (term_id:6 doc: 2 freq: 1)],\n",
       " 'o': [(term_id:7 doc: 3 freq: 1)],\n",
       " 'pr√©dio': [(term_id:8 doc: 3 freq: 1)]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"a\": [TermOccurrence(1,1,1), TermOccurrence(2,1,1)],\n",
    " \"casa\": [TermOccurrence(1,2,2), TermOccurrence(2,2,1)],\n",
    " \"verde\": [TermOccurrence(1,3,1), TermOccurrence(3,3,1)],\n",
    " \"√©\": [TermOccurrence(1,4,1)],\n",
    " \"uma\": [TermOccurrence(1,5,1)],\n",
    " \"bonita\": [TermOccurrence(1,6,1),TermOccurrence(2,6,1)],\n",
    " \"o\": [TermOccurrence(3,7,1)],\n",
    " \"pr√©dio\": [TermOccurrence(3,8,1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidade deste indice, perceba que deixamos o `term_id` de forma repetida. Iremos deixar assim, por√©m poderiamos retirar essa redundancia para reduzir o consumo de mem√≥ria. Por√©m, nesta pr√°tica, iremos implementar o √≠ndice em arquivo que ir√° ser melhor ainda na quest√£o de consumo de mem√≥ria ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O √≠ndice √© chamado da seguinte forma - esse c√≥digo s√≥ ira funcionar depois que voc·∫Ω terminar esta atividade :):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = HashIndex()\n",
    "#indexa√ß√£o do documento 1\n",
    "index.index(\"a\",1,1)\n",
    "index.index(\"casa\",1,2)\n",
    "index.index(\"verde\",1,1)\n",
    "index.index(\"√©\",1,1)\n",
    "index.index(\"uma\",1,1)\n",
    "#indexa√ß√£o do documento 2\n",
    "index.index(\"a\",2,1)\n",
    "index.index(\"casa\",2,1)\n",
    "index.index(\"bonita\",2,1)\n",
    "\n",
    "#indexa√ß√£o do documento 3\n",
    "index.index(\"o\",3,1)\n",
    "index.index(\"pr√©dio\",3,1)\n",
    "index.index(\"verde\",3,1)\n",
    "\n",
    "index.finish_indexing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `Index` √© que ir√° manter o dicin√°rio com o vocabul√°rio do √≠ndice e, dependendo de sua implementa√ß√£o, suas ocorrencias. Iremos fazer duas implementa√ß√µes: a classe `HashIndex`, que faz o indice e suas ocorrencias em mem√≥ria principal e a classe `FileIndex`, que armazena as ocorr√™ncias em arquivo, ambas subclasses de `Index`. O m√©todo `finish_indexing` √© um m√©todo que n√£o est√° implementado no `Index` e √© implementado (opcionalmente) nas suas subclasses caso haja necessidade de fazer algo no final da indexa√ß√£o. Em nosso caso, apenas a classe `FileIndex` ir√° precisar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, voc√™ dever√° implementar a classe HashIndex. Para sua implementa√ß√£o, voc√™ dever√° completar os seguintes m√©todos/atributo calculados:\n",
    "- `create_index_entry`: cria uma nova entrada no √≠ndice utilizando, se necess√°rio, o id do termo passado como par√¢metro - n√£o ser√° necess√°rio agora. A implementa√ß√£o deste m√©todo √© **super simples** - apenas substitua o None. Verifique tamb√©m o m√©todo `index` da superclasse para entender melhor o que ser√° retornado;\n",
    "- `add_index_occur`: Adiciona uma nova ocorrencia na entrada deste √≠ndice. Voc√™ precisar√° da entrada do termo atual, o id do documento e frequencia do termo no documento, passado como par√¢metro. Atualize substituindo os `None`. Veja tamb√©m o modo `index` da superclasse para entender melhor como o m√©todo funciona.\n",
    "\n",
    "Fa√ßa os testes baixo para garantir que a atividade atual e as duas anteriores foram implementadas corretamente. Nos primeiros dois testes, voc√™ ainda n√£o ver√° a lista de ocorr√™ncias, pois o m√©todo para obt√™-la ser√° implementado a seguir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Indice Gerado ======\r\n",
      "casa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\r\n",
      "vermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\r\n",
      "verde -> [(term_id:2 doc: 1 freq: 1)]\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Indice Gerado ======\r\n",
      "casa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\r\n",
      "vermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\r\n",
      "verde -> [(term_id:2 doc: 1 freq: 1)]\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_document_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, implemente os m√©todos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_occurrence_list`: Retornar√° a lista de ocorrencias de um determinado termo. Considerando o exemplo apresentado no inicio desta atividade, `index.get_occurrence_list('casa')` retornar√° a lista `[TermOccurrence(1,2,2), TermOccurrence(2,2,1)]`. Caso um termo n√£o exista, este m√©todo dever√° retornar uma lista vazia. Logo ap√≥s, execute o teste abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Indice Gerado ======\r\n",
      "casa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\r\n",
      "vermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\r\n",
      "verde -> [(term_id:2 doc: 1 freq: 1)]\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_get_occurrence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `document_count_with_term`: Retorna a quantidade de documentos que possuem um determinado termo. Considerando o exemplo apresentado no inicio desta atividade, `index.document_count_with_term('casa')` retornar√° 2. Caso um termo n√£o exista, este m√©todo dever√° retornar zero. Logo ap√≥s, execute o teste abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Indice Gerado ======\n",
      "casa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\n",
      "vermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\n",
      "verde -> [(term_id:2 doc: 1 freq: 1)]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.index_structure_test StructureTest.test_document_count_with_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 - m√©todos de compara√ß√£o da classe TermOccurrence**: Eventualmente iremos precisar ordenar as ocorr√™ncias Por isso, temos que implementar os [comparadores de `__eq__` e `__lt__`](https://docs.python.org/3.7/reference/datamodel.html#object.__lt__) al√©m de usar o _decorator_ [total_ordering](https://docs.python.org/3.7/library/functools.html#functools.total_ordering) - [veja tamb√©m aqui](https://portingguide.readthedocs.io/en/latest/comparisons.html#rich-comparisons). `__eq__` retorna igual se um objeto √© considerado igual ao outro. Considere que uma ocorrencia √© igual a outra se o id do termo dela e o id do documento forem iguais. \n",
    "\n",
    "O comparador `<` √© implementado pelo m√©todo `__lt__` que retorna verdadeiro sde o objeto corrrente `self` √© menor do que o objeto passado como parametro. A ocorrencia dever√° ser ordenada primeiramente pelo seu `term_id` e, logo ap√≥s, pelo `doc_id`. Fa√ßa o exemplo abaixo para testar (n√£o esque√ßa de reiniciar o Kernel quando modificar o c√≥digo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado obtido: False - esperado: False\n",
      "Resultado obtido: True - esperado: True\n",
      "Resultado obtido: False - esperado: False\n",
      "Resultado obtido: False - esperado: False\n",
      "Resultado obtido: True - esperado: True\n",
      "Resultado obtido: False - esperado: False\n",
      "Resultado obtido: True - esperado: True\n",
      "Resultado obtido: False - esperado: False\n",
      "Resultado obtido: False - esperado: False\n"
     ]
    }
   ],
   "source": [
    "from index.structure import *\n",
    "t1 = TermOccurrence(1,1,2)\n",
    "t2 = TermOccurrence(3,1,2)\n",
    "t3 = TermOccurrence(1,2,2)\n",
    "t4 = TermOccurrence(2,2,2)\n",
    "t5 = TermOccurrence(2,2,2)\n",
    "\n",
    "\n",
    "print(f\"Resultado obtido: {t1 == t5} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t4 == t5} - esperado: True\")\n",
    "print(f\"Resultado obtido: {t1 != t1} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t1 == None} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t1 < t2} - esperado: True\")\n",
    "print(f\"Resultado obtido: {t2 > t3} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t3 < t4} - esperado: True\")\n",
    "print(f\"Resultado obtido: {t2 > t4} - esperado: False\")\n",
    "print(f\"Resultado obtido: {t2 > None} - esperado: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constru√ß√£o de √≠ndice usando arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constru√ß√£o de √≠ndice usando apenas mem√≥ria principal f√°cil de implementar e eficiente em termos de tempo de execu√ß√£o. Por√©m, quando precisamos de indexar milh√µes/bilh√µes de p√°ginas, √© muitas vezes invi√°vel armazenarmos tudo em mem√≥ria principal. \n",
    "\n",
    "Para resolver esse problema, uma solu√ß√£o √© mantermos o vocabul√°rio em mem√≥ria principal e as ocorr√™ncias em mem√≥ria secund√°ria. Assim, teriamos o mesmo atributo `dic_index` na classe `Index`. Por√©m, cada entrada (termo) referenciar√° as ocorrencias em arquivo. Utilizando exemplo da atividade 3 neste contexto, no final da indexa√ß√£o, o `dic_index` deve ficar da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': term_id: 1, doc_count_with_term: 2, term_file_start_pos: 0,\n",
       " 'casa': term_id: 2, doc_count_with_term: 2, term_file_start_pos: 20,\n",
       " 'verde': term_id: 3, doc_count_with_term: 2, term_file_start_pos: 40,\n",
       " '√©': term_id: 4, doc_count_with_term: 1, term_file_start_pos: 60,\n",
       " 'uma': term_id: 5, doc_count_with_term: 1, term_file_start_pos: 70,\n",
       " 'bonita': term_id: 6, doc_count_with_term: 2, term_file_start_pos: 80,\n",
       " 'o': term_id: 7, doc_count_with_term: 1, term_file_start_pos: 100,\n",
       " 'pr√©dio': term_id: 8, doc_count_with_term: 1, term_file_start_pos: 110}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"a\": TermFilePosition(1, 0, 2), \n",
    " \"casa\": TermFilePosition(2, 20, 2), \n",
    " \"verde\": TermFilePosition(3, 40, 2),\n",
    " \"√©\": TermFilePosition(4, 60, 1), \n",
    " \"uma\": TermFilePosition(5, 70, 1), \n",
    " \"bonita\": TermFilePosition(6, 80, 2), \n",
    " \"o\": TermFilePosition(7, 100, 1), \n",
    " \"pr√©dio\": TermFilePosition(8, 110, 1), \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e as ocorrencias, ordenadas por termo e, logo ap√≥s, por documento ficariam em um arquivo na seguinte ordem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(term_id:1 doc: 1 freq: 1),\n",
       " (term_id:1 doc: 2 freq: 1),\n",
       " (term_id:2 doc: 1 freq: 2),\n",
       " (term_id:2 doc: 2 freq: 1),\n",
       " (term_id:3 doc: 1 freq: 1),\n",
       " (term_id:3 doc: 3 freq: 1),\n",
       " (term_id:4 doc: 1 freq: 1),\n",
       " (term_id:5 doc: 1 freq: 1),\n",
       " (term_id:6 doc: 1 freq: 1),\n",
       " (term_id:6 doc: 2 freq: 1),\n",
       " (term_id:7 doc: 3 freq: 1),\n",
       " (term_id:8 doc: 3 freq: 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TermOccurrence(1,1,1), \n",
    " TermOccurrence(2,1,1), \n",
    " TermOccurrence(1,2,2), \n",
    " TermOccurrence(2,2,1),\n",
    " TermOccurrence(1,3,1), \n",
    " TermOccurrence(3,3,1),\n",
    " TermOccurrence(1,4,1),\n",
    " TermOccurrence(1,5,1),\n",
    " TermOccurrence(1,6,1),\n",
    " TermOccurrence(2,6,1),\n",
    " TermOccurrence(3,7,1),\n",
    " TermOccurrence(3,8,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "em que cada instancia da classe `TermFilePosition` √© a especifica√ß√£o da posi√ß√£o inicial de um `term_id` em um arquivo  al√©m de especificar tamb√©m a quantidade de ocorrencias desse termo. Essa posi√ß√£o inicial e quantidade s√£o definidas nos atributos atributos `term_file_start_pos` e `doc_count_with_term`, respectivamente. A posi√ß√£o inicial est√° em bytes e, nesse exemplo, foi considerado que cada TermOcurrence possui 10 bytes. Assim, por exemplo, o termo casa (term_id=2) inicia-se na posi√ß√£o 20 e possui duas ocorr√™ncias. Com isso, √© poss√≠vel obter todas as ocorrencias de um determinado termo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para deixarmos a estrutura dessa forma, temos um dificultador: no arquivo, temos que ordenar as ocorrencias pelo termo, por√©m, indexamos por documento (veja na atividade 3). Assim, se grav√°ssemos as ocorr√™ncias assim que indexarmos, as gravariamos agrupadas por documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, temos que garantir uma ordena√ß√£o por termo do arquivo externo, lembrando que nem sempre √© poss√≠vel armazenar todo o arquivo em mem√≥ria principal. Para resolvermos isso, faremos o seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sempre, ao indexar, salvaremos o indice em uma lista (tempor√°ria) de ocorrencia de termos `lst_occurrences_tmp`\n",
    "- Usaremos o m√©todo `save_tmp_occurrences` para, assim que a lista estiver com um determinado tamanho, orden√°-la pelo termo e salvar de formar ordenada em um novo arquivo de indice com a todas as ocorrencias. Para que seja feito isso, voc√™ dever√° fazer uma ordena√ß√£o externa lendo m considera√ß√£o o √≠ndice em arquivo atual e a lista de ocorrencias tempor√°rias. Segue o passo a passo:\n",
    "    - (1) ordene a lista `lst_occurrences_tmp`. Lembre-se que voc√™ implementou os comparadores das instancias TermOccurrence, assim, a ordena√ß√£o e descobrir o menor valor entre as ocorrencias √© uma opera√ß√£o simples;\n",
    "    - (2) criar um arquivo novo;\n",
    "    - (3) compare a primeira posi√ß√£o da lista com a primeira posi√ß√£o do arquivo de √≠ndice, sempre inserindo a ocorrencia considerada com o menor entre elas no novo arquivo. Lembrando novamente que os comparadores foram implementados e que voc√™ possui os m√©todos `next_from_list` e `next_from_file` - que ser√° implementado na atividade 5(b) para ajudar;\n",
    "    - (4) esse novo arquivo passar√° a ser o √≠ndice. Exclua o indice antigo e limpe a lista de ocorrencias `lst_occurrences_tmp`.\n",
    "- O m√©todo `finish_indexing` √© o m√©todo que ser√° chamado ao finalizar a indexa√ß√£o. Neste contexto ele ser√° usado para organizar o `dic_index` atualizando os atributos `term_file_start_pos` e `doc_count_with_term` para os valores corretos.\n",
    "\n",
    "Na primeira execu√ß√£o, n√£o haver√° arquivo e voc√™ adicionar√° a lista toda no arquivo de forma sequencial. Fazendo esse procedimento, voc√™ sempre ir√° garantir um arquivo ordenado da forma esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5(a) - m√©todo de escrita no arquivo:** Iremos necessitar, em algum momento, a escrita de uma instancia de `TermOccurrence` em arquivo. Assim deveremos implementar o m√©todo de escrita em arquivo nessa classe. Para economizar espa√ßo e por simplicidade, ser√° escrito em um arquivo bin√°rio armazenando os tr√™s atributos inteiros. Cada inteiro ser√° armazenado em 4 bytes - ser√° o suficiente para a nossa indexa√ß√£o. Veja um exemplo abaixo de escrita, leitura e impress√£o do posicionamento no arquivo (esses m√©todos ser√£o uteis nessa e nas pr√≥ximas atividades)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "n√∫mero: 100\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "with open(\"xuxu.idx\",\"wb\") as file:\n",
    "    print(file.tell())\n",
    "    file.write(x.to_bytes(4,byteorder=\"big\"))\n",
    "    print(file.tell())\n",
    "with open(\"xuxu.idx\",\"rb\") as file:\n",
    "    print(f\"n√∫mero: {int.from_bytes(file.read(4),byteorder='big')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5(b) - m√©todos next_from_file e next_from_list**: Antes de fazer a ordena√ß√£o, √© interessante implementar esses dois m√©todos que ir√£o auxiliar na obten√ß√£o do menor elemento no arquivo e na lista, respectivamente. Considerando que a lista e o arquivo est√£o ordenados de forma crescente, temos que obter o primeiro elemento da lista e retir√°-lo (utilize o [m√©todo pop](https://docs.python.org/3.1/tutorial/datastructures.html)).  \n",
    "\n",
    "Para a leitura do arquivo, iremos usar a API [pickle](https://docs.python.org/3/library/pickle.html) que facilita inserir/carregar estruturas em arquivo bin√°rio. O m√©todo `next_from_file` j√° possui um arquivo aberto e voc√™ dever√° ler a pr√≥xima entrada por meio da fun√ß√£o `load` da API `pickle`. Caso n√£o exista pr√≥ximo elemento, √© lan√ßado uma exce√ß√£o. Em ambos os m√©todos, caso n√£o exista pr√≥ximo elemento, ser√° retonado `None`. \n",
    "\n",
    "Complete a implementa√ß√£o substituindo os `None` quando julgar necess√°rio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7 - m√©todo save_tmp_occurrences: ** Implemente o m√©todo `save_tmp_occurrences`. Esse m√©todo dever√° salvar a lista `lst_occurrences_tmp` em arquivo de forma ordenada, conforme explicado anteriormente. Neste m√©todo, voc√™ n√£o precisa preocupar com o atributo `dic_index`. Leve em considera√ß√£o os seguintes atributos/m√©todos:\n",
    "\n",
    "- `idx_file_counter`:  No c√≥digo, voc√™ ir√° criar sempre novos indices, excluindo o antigo. Este atributo ser√° √∫til para definirmos o nome do arquivo do √≠ndice. O novo arquivo do √≠ndice chamar√° `occur_index_X` em que $X$ √© o n√∫mero do mesmo. \n",
    "- `str_idx_file_name`: Atributo que armazena o arquivo indice atual. A primeira vez que executarmos `save_tmp_occurrences` n√£o haver√° arquivo criado e, assim `str_idx_file_name = None`\n",
    "- `lst_occurrences_tmp`: Lista de ocorrencias a serem armazenadas em arquivo\n",
    "- `next_from_file` e `next_from_list`: implementados na atividade anterior, para obter o proximo item do arquivo ou da lista. Importantes para fazer a ordena√ß√£o.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unit√°rio abaixo para verificar corretude deste c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeira execu√ß√£o (cria√ß√£o inicial do indice) [ok]\n",
      "Inser√ß√£o de alguns itens - teste 1/2 [ok]\n",
      "Inser√ß√£o de alguns itens - teste 2/2 [ok]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.017s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.file_index_test FileIndexTest.test_save_tmp_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8: m√©todo finish_indexing: ** Agora, com as ocorr√™ncias organizadas no arquivo por termo, voc√™ dever√° implementar o m√©todo `finish_indexing` para atualizar o atributo `dic_index` com a posi√ß√£o inicial e quantidade de documentos de cada termo nas suas instancias `TermFilePosition`. Logo ap√≥s, execute o teste unit√°rio abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de ocorr√™ncias a serem testadas:\r\n",
      "(term_id:1 doc: 1 freq: 3)\r\n",
      "(term_id:1 doc: 2 freq: 2)\r\n",
      "(term_id:1 doc: 3 freq: 1)\r\n",
      "(term_id:2 doc: 1 freq: 1)\r\n",
      "(term_id:2 doc: 2 freq: 1)\r\n",
      "(term_id:2 doc: 3 freq: 2)\r\n",
      "(term_id:3 doc: 1 freq: 3)\r\n",
      "(term_id:4 doc: 1 freq: 5)\r\n",
      "(term_id:4 doc: 2 freq: 5)\r\n",
      "Tamanho de cada ocorr√™ncia: 94 bytes\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.006s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.file_index_test FileIndexTest.test_finish_indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 - implementa√ß√£o em `FileIndex` dos m√©todos abstratos da classe Index: ** Como voc√™s perceberam, `FileIndex` √© subclasse de `Index`. Assim,  precisamos implementar os m√©todos abstratos da classe `Index`:\n",
    "\n",
    "- `create_index_entry`: no `FileIndex` para criar uma nova entrada no √≠ndice, voc√™ dever√° retornar uma instancia de `TermFilePosition` para este novo `term_id`. Ao cri√°-lo, voc√™ n√£o precisa de definir a posi√ß√£o inicial do arquivo nem a quantidade de documentos. Conforme voc√™s implementaram nas atividades anteriores, isso √© feito apenas no momento de finaliza√ß√£o da indexa√ß√£o;\n",
    "- `add_index_occur`: Neste caso, voc√™ dever√° criar e adicionar uma nova ocorrencia na lista de ocorrencias tempor√°rias `lst_occurrences_tmp` e, caso senha passado o limite `FileIndex.TMP_OCCURRENCES_LIMIT` do n√∫mero m√°ximo de ocorrencias na lista, chamar o m√©todo `save_tmp_occurrence`.\n",
    "- `get_occurrence_list` e `document_count_with_term`: Possuem as mesmas funcionalidades descritas na atividade 3, por√©m, agora voc√™ dever√° considerar a estrutura criada no `FileIndex`. Lembrem-se que esse m√©todo √© s√≥ chamado ap√≥s a finaliza√ß√£o da indexa√ß√£o, assim, considere que o √≠ndice j√° est√° pronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 10 - Teste unit√°rio** Dessa vez, voc√™ dever√° alterar uma classe de teste unit√°rio para conseguir execut√°-la. \n",
    "\n",
    "Agora iremos testar os m√©todos get_occurrence_list e document_count_with_term. Lembre-se que j√° temos um teste unit√°rio para isso, por√©m ele testa a estrutura de um `HashIndex`. Neste caso, iremos apenas mudar a estrutura, mas os m√©todos ser√£o o mesmo, por isso, conseguiremos reaproveitar o teste feito anteriormente criando apenas uma nova classe de teste. \n",
    "\n",
    "Implementando este teste, voc√™ perceber√° como √© lindo usar orienta√ß√£o objetos ao seu favor ü•∞. No arquivo `index_structure_test.py` voc√™ possui a classe `FileStructureTest` que √© subclasse de nosso teste criado `StructureTest`.  Voc√™ dever√° implementar o m√©todo `setUp` na classe `FileStructureTest` que sobrep√µe o m√©todo de mesmo nome na classe `StructureTest`. O m√©todo `setUp` √© executado sempre antes do teste. Este m√©todo que voc√™ ir√° criar ir√° fazer exatamente a mesma coisa que o criado em `StructureTest` por√©m, voc√™ dever√° instanciar um `FileIndex` ao inv√©s de um `HashIndex`. Logo ap√≥s, execute os testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Indice Gerado ======\r\n",
      "casa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\r\n",
      "vermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\r\n",
      "verde -> [(term_id:2 doc: 1 freq: 1)]\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.004s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.index_structure_test  FileStructureTest.test_document_count_with_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Indice Gerado ======\r\n",
      "casa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\r\n",
      "vermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\r\n",
      "verde -> [(term_id:2 doc: 1 freq: 1)]\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.003s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m index.index_structure_test  FileStructureTest.test_get_occurrence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar, tamb√©m criamos um teste de performance. Verifique a performance da indexa√ß√£o em arquivo e em mem√≥ria ao indexar milh√µes de ocorrencias de termos utilizando os testes abaixo. Note que, desta vez, estamso chamando os testes por meio de comandos Python e n√£o pelo terminal. Assim, caso queira fazer alguma altera√ß√£o no arquivos `.py`, voc√™ deve reiniciar o kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- √çndice completamente em mem√≥ria principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria usada: 203.545836 MB; M√°ximo 203.54598 MB\n",
      "Indexando ocorrencia #1,250,000/1,250,000 (100%)\n",
      "Tempo gasto: 8.336006s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 9.031s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from index.performance_test import PerformanceTest\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 2500\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 500\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(PerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- √çndice com ocorrencias em mem√≥ria secund√°ria. Veja que, abaixo, que voc√™ pode ajustar o parametro de n√∫mero de ocorr√™ncias em mem√≥ria. Ser√° muito √∫til para n√£o gastar tanto tempo ao indexar o conte√∫do da Wikip√©dia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria usada: 4.27021 MB; M√°ximo 5.909258 MB\n",
      "Indexando ocorrencia #100,000/100,000 (100%)\n",
      "Tempo gasto: 9.052715s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 9.068s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from index.performance_test import FilePerformanceTest,PerformanceTest\n",
    "from index.structure import FileIndex\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 10\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 10000\n",
    "FileIndex.TMP_OCCURRENCES_LIMIT = 10000\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(FilePerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria usada: 4.278269 MB; M√°ximo 5.921208 MB\n",
      "Indexando ocorrencia #500,000/500,000 (100%)\n",
      "Tempo gasto: 233.691548s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 233.707s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from index.performance_test import FilePerformanceTest,PerformanceTest\n",
    "from index.structure import FileIndex\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 10\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 50000\n",
    "FileIndex.TMP_OCCURRENCES_LIMIT = 10000\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(FilePerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo ap√≥s executado este teste, voc√™ dever√° usar a biblioteca [JSON](https://docs.python.org/3/library/json.html) ou [Pickle](https://docs.python.org/3/library/pickle.html) para armazenar o vocabul√°rio. Com isso, crie um m√©todo de leitura do FileIndex e de escrita. O m√©todo de leitura dever√° ser um m√©todo estatico que retorna um objeto da classe indice previamente criado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexador de HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, voc√™ ir√° alterar o arquivo `indexer.py` para [preprocessar conte√∫do HTML](https://docs.google.com/presentation/d/1C22jQWIYobiqMx8SmP1y2lr1uSlvJSu3ayu5lXC5d8A/edit?usp=sharing) e depois index√°-lo. Com isso, voc√™ poder√° us√°-lo para indexa√ß√£o das p√°ginas HTML, como os da Wikip√©dia. A classe `Cleaner` ser√° respons√°vel pelo preprocessamento e a HTMLIndexer, para a indexa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/jonathan.candido/miniforge3/lib/python3.9/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /Users/jonathan.candido/miniforge3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: click in /Users/jonathan.candido/miniforge3/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/jonathan.candido/miniforge3/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/jonathan.candido/miniforge3/lib/python3.9/site-packages (from nltk) (4.60.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/Users/jonathan.candido/miniforge3/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonathan.candido/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from index.indexer import *\n",
    "#importamos o m√≥dulo structure\n",
    "#novamente para n√£o precisar de executar o c√≥digo do in√≠cio da tarefa\n",
    "from index.structure import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 11 - Limpeza dos dados com a classe Cleaner: ** A classe `Cleaner` √© respons√°vel por preprocessar o conte√∫do HTML para que ele esteja preparado para indexa√ß√£o. Essa classe tem alguns _flags_ para definir se algum tipo de processamento opcional ser√° feito (por exemplo, _stemming_ e remo√ß√£o de _stopwords_). Para isso, voc√™ dever√° implementar pequenos m√©todos para fazer a limpeza. Esses c√≥digos s√£o pequenos pois temos lindas APIs para nos ajudar üíï. Voc√™ ir√° fazer o processamento b√°sico e, se quiser, pode melhorar a implementa√ß√£o criando exce√ß√µes na remo√ß√£o de acentos e n√£o retirando mai√∫sculas e min√∫sculas de certas palavras e unindo palavras compostas, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada tarefa, h√° um m√©todo para ser criado a seguir os testes iniciais ser√£o feitos aqui no Jupyter. N√£o esque√ßa de reiniciar o kernel sempre que alterar algo no c√≥digo. Logo ap√≥s, haver√° um [teste de integra√ß√£o](https://en.wikipedia.org/wiki/Integration_testing) para avaliar a indexa√ß√£o como um todo.\n",
    "\n",
    "- **Transforma√ß√£o de HTML para texto: ** Na limpeza dos dados, iremos remover tudo que n√£o ser√° indexado - ou seja, o c√≥digo HTML. Para isso, iremos implementar o m√©todo `html_to_plain_text` que transformar√° o HTML em texto corrido. Voc√™ pode utilizar o [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) para isso e o m√©todo get_text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬© oi! Meu nome √© Hasan'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner_test = Cleaner(stop_words_file=\"stopwords.txt\",\n",
    "                        language=\"portuguese\",\n",
    "                        perform_stop_words_removal=True,\n",
    "                        perform_accents_removal=True,\n",
    "                        perform_stemming=True)\n",
    "cleaner_test.html_to_plain_text(\"&copy; oi! Meu nome √© <strong>Hasan</strong>\")\n",
    "#esperado: '¬© oi! Meu nome √© Hasan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Verifica se √© stopword**: O m√©todo `is_stopword` retorna verdadeiro se uma palavra, passada como par√¢metro, √© stopword. Para isso, voc√™ ir√° usar o atributo `set_stop_words`. Este atibuto foi inicializado com um conjunto de stopwords de um arquivo. Esse arquivo, para testes, tem poucas stopwords. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False, esperado: False\n",
      "False, esperado: False\n",
      "True, esperado: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cleaner_test.is_stop_word('jap√£o')}, esperado: False\")\n",
    "print(f\"{cleaner_test.is_stop_word('cama')}, esperado: False\")\n",
    "print(f\"{cleaner_test.is_stop_word('√©')}, esperado: True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stemming**: voc√™ dever√° implementar o m√©todo word_stem para ralizar o stemming. Voc√™ dever√° usar a [classe SnowballStemmer da API NLTK](https://www.nltk.org/howto/stem.html). Um objeto dessa classe j√° est√° instanciado no atributo `stemmer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verdad, esperado: verdad\n",
      "estud, esperado: estud\n",
      "amad, esperado: amad\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cleaner_test.word_stem('verdade')}, esperado: verdad\")\n",
    "print(f\"{cleaner_test.word_stem('estudante')}, esperado: estud\")\n",
    "print(f\"{cleaner_test.word_stem('amado')}, esperado: amad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remo√ß√£o de acentos:** Iremos fazer de uma forma bem simples a remo√ß√£o de acentos: aplicando uma tabela de substitui√ß√£o de caracteres. Para isso, voc√™ dever√° criar uma [tabela de tradu√ß√£o](https://docs.python.org/3.3/library/stdtypes.html?highlight=maketrans#str.maketrans) no atributo `accents_translation_table` baseando-se nas vari√°veis `in_table` e `out_table` tamb√©m presentes no construtor (substitua o None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancao, esperado: cancao\n",
      "eletrico, esperado: eletrico\n",
      "amado, esperado: amado\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cleaner_test.remove_accents('can√ß√£o')}, esperado: cancao\")\n",
    "print(f\"{cleaner_test.remove_accents('el√©trico')}, esperado: eletrico\")\n",
    "print(f\"{cleaner_test.remove_accents('amado')}, esperado: amado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora voc√™ ir√° fazer o m√©todo `preprocess_word` ele ir√° receber como parametro uma palavra e ir√° verificar se √© uma palavra v√°lida de ser indexada. Caso n√£o seja, retornar√° None. Caso contr√°rio, ir√° retornar a palvra preprocessada. Uma palavra v√°lida a ser indexada √© aquela que n√£o √© pontua√ß√£o e n√£o √© stopword (caso `perform_stop_words_removal = True`). Para que seja feito o preprocessamento voc√™ dever√°: transformar o texto para min√∫sculas, remover acento (se `perform_accents_removal=True`), fazer o stemming (se `perform_stemming = True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 12 - Classe HTMLIndexer - m√©todo text_word_count: **  Voc√™ dever√° implementar o m√©todo `text_word_count`, a partir de um texto. Esse m√©todo retorna um dicion√°rio em que, para cada palavra no texto, ser√° apresentado sua frequ√™ncia. Considere que o texto j√° est√° limpo e √© necess√°rio fazer apenas o processamento das palavras.\n",
    "\n",
    "Para isso, voc√™ dever√°:  dividir o texto em tokens (que, no nosso caso, s√£o as palavras e pontua√ß√µes); preprocessar cada palavra usando o `HTMLIndexer.cleaner`; e, se for uma palavra v√°lida, contabiliz√°-la. Para isso, ser√° necess√°rio [o m√©todo word_tokenize da API NLTK](https://kite.com/python/docs/nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ola': 1, 'qual': 1, 'o': 1, 'dad': 2, 'que': 1, 'precis': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = HashIndex()\n",
    "indexador_teste = HTMLIndexer(index)\n",
    "indexador_teste.text_word_count(\"Ol√°! Qual √© o dado dado que precisa?\")\n",
    "#esperado:\n",
    "#{'dad': 2, 'o': 1, 'ola': 1, 'precis': 1, 'qual': 1, 'que': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 13 - m√©todo index_text: ** Implemente o m√©todo `index_text` que dever√° (1) converter o HTML para texto simples usando `HTMLIndexer.cleaner`; (2) converter o texto em um dicion√°rio de ocorrencias de palavras com sua frequencia (metodo da atividade 12); e (3) indexar cada palavra deste dicion√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = HashIndex()\n",
    "indexador_teste = HTMLIndexer(index)\n",
    "#o HTML est√° mal formado de prop√≥sito ;)\n",
    "indexador_teste.index_text(10,\"<strong>Ol&aacute;! </str> Quais s√£o os dados que precisar√°?\")\n",
    "\n",
    "indexador_teste.index.dic_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperado:\n",
    "<pre>\n",
    "{'dad': [(term_id:4 doc: 10 freq: 1)],\n",
    " 'ola': [(term_id:0 doc: 10 freq: 1)],\n",
    " 'os': [(term_id:3 doc: 10 freq: 1)],\n",
    " 'precis': [(term_id:6 doc: 10 freq: 1)],\n",
    "'qua': [(term_id:1 doc: 10 freq: 1)],\n",
    " 'que': [(term_id:5 doc: 10 freq: 1)],\n",
    " 'sao': [(term_id:2 doc: 10 freq: 1)]}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 14: Indexa√ß√£o de um diretorio com subdiretorios** Voc√™ dever√° implementar o m√©todo `index_text_dir` que, dado um diretorio, navega em todos os seus subdiret√≥rios e indexa todos os arquivos HTMLs. Considere que os arquivos sejam sempre nomeados pelo seu ID. Veja o exemplo em `doc_test`. Logo ap√≥s, execute o teste unit√°rio para ver a corretude do seu indexador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m index.indexer_test IndexerTest.test_indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para fazer a especifica√ß√£o do projeto, voc√™ deve baixar o dataset da Wikip√©dia e index√°-lo. Voc√™ deve tamb√©m achar um arquivo de stopwords com mais termos aos que feito neste teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonathan.candido/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from index.indexer import *\n",
    "#importamos o m√≥dulo structure\n",
    "#novamente para n√£o precisar de executar o c√≥digo do in√≠cio da tarefa\n",
    "from index.structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'das', 'estivermos', 'qual', 'houveram', 'houver√≠amos', 'nossa', 'seus', 'lhes', 'teve', 'por', 'hei', 'estiv√©ramos', 'houve', 'houv√©ramos', 'houvessem', 'voc√™', 'do', 'te', 'aquele', 'n√≥s', '√©', 'num', 'estiverem', 'hajamos', 'houveria', 'mais', 't√≠nhamos', 'est√°vamos', 'foram', 'seriam', 'era', '√†s', 'esta', 'as', 'estou', 'houvermos', 'fui', 'est√°', 'tiveram', 'tenhamos', 'pelo', 'os', 'com', 'deles', 'pelas', 'sejam', 'est√£o', 'aquelas', 'nossos', 't√©m', 'isto', 'j√°', 'tiver', 'aquela', 'n√£o', 'pelos', 'dele', 'ele', 'tivemos', 'tinham', 'tenham', 'numa', 'tu', 'esses', 'tenho', 'houvera', 'na', 'ela', 'foi', 'de', 'estava', 'teremos', 'sem', 'tenha', 'entre', 'at√©', 'nas', 'aquilo', 'teu', 'aqueles', 'esteja', 'forem', 'tivera', 'houveriam', 'o', 'fosse', 'esse', 'ser√≠amos', 'pela', 'fomos', 'eram', 'estivemos', 'tiv√©ssemos', 'houver√£o', 'tivessem', 'vos', 'eu', 'eles', 'meus', 'essa', 'ou', 'teus', 'se', 'dela', 'tivermos', 'seria', 'um', 'terei', 'houveremos', 'estiveram', 'havia', 'isso', 'este', 'fossem', 'estavam', 'me', 'nos', 'nosso', 'ter', 'a', 's√£o', 'e', 'tiv√©ramos', 'houver√°', 'f√¥ramos', 'lhe', 'seja', 'for', 'teriam', 'estiver', 'houverem', 'ao', 'estive', 'sou', 'seremos', 'houvesse', 'em', 'dos', 'temos', 'depois', 'quem', 'houver', 'mas', 'ser√°', '\\n', 'quando', 'da', 'delas', 'essas', 'tuas', 'uma', 's√≥', 'estas', 'muito', 't√™m', 'somos', 'mesmo', 'estivesse', 'serei', 'voc√™s', 'no', 'minhas', 'formos', 'tua', '√©ramos', 'f√¥ssemos', 'houvemos', 'ter√°', 'ser', 'sejamos', 'minha', 'teria', 'tivesse', 'nossas', 'fora', 'tive', 'estejam', 'como', 'sua', 'elas', 'estes', 'que', 'hajam', 'tinha', 'estivera', 'estamos', 'estivessem', 'ter√£o', 'nem', 'ser√£o', 'ter√≠amos', 'houverei', 'para', 'tem', '√†', 'aos', 'suas', 'meu', 'haja', 'seu', 'estejamos', 'tiverem', 'esteve', 'houv√©ssemos', 'estiv√©ssemos', 'havemos', 'h√°', 'h√£o', 'tamb√©m'}\n"
     ]
    }
   ],
   "source": [
    "## lista de stopwords retirada de: https://gist.github.com/alopes/5358189\n",
    "cleaner_test = Cleaner(stop_words_file=\"./wiki/stopwords.txt\",\n",
    "                        language=\"portuguese\",\n",
    "                        perform_stop_words_removal=True,\n",
    "                        perform_accents_removal=True,\n",
    "                        perform_stemming=True)\n",
    "print(cleaner_test.set_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f96e5f69ff64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhtml_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhtml_indexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_text_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index/wiki/wikiSample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_traced_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/www/recuperacao-da-informacao/exercises/indexador/index/indexer.py\u001b[0m in \u001b[0;36mindex_text_dir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                         \u001b[0mintit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                         \u001b[0mspend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/www/recuperacao-da-informacao/exercises/indexador/index/indexer.py\u001b[0m in \u001b[0;36mindex_text\u001b[0;34m(self, doc_id, text_html)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_html\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLIndexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_to_plain_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mdic_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_word_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/www/recuperacao-da-informacao/exercises/indexador/index/indexer.py\u001b[0m in \u001b[0;36mhtml_to_plain_text\u001b[0;34m(self, html_doc)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhtml_to_plain_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# < + letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/html/parser.py\u001b[0m in \u001b[0;36mparse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# Now parse the data between i+1 and j into a tag and attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagfind_tolerant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unexpected call to parse_starttag()'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from index.indexer import *\n",
    "from index.structure import *\n",
    "import tracemalloc\n",
    "time_first = datetime.now()\n",
    "obj_index = FileIndex()\n",
    "html_indexer = HTMLIndexer(obj_index)\n",
    "tracemalloc.start()\n",
    "html_indexer.index_text_dir(\"index/wiki/wikiSample\")\n",
    "current, peak = tracemalloc.get_traced_memory()            \n",
    "time_end = datetime.now()\n",
    "tempo_gasto = time_end-time_first \n",
    "print(f\"Memoria usada: {current / 10**6:,} MB; M√°ximo {peak / 10**6:,} MB\")\n",
    "print(f\"Finalizado:{tempo_gasto.total_seconds()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd arquivos: 1 Soma total: 0.04697 media:0.04697 segundos\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "soma = 0\n",
    "with open(\"times.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        a+=1\n",
    "        soma+=  float(line.split(\":\")[1])\n",
    "\n",
    "    print(f\"Qtd arquivos: {a} Soma total: {soma} media:{(soma/a)} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(term_id:0 doc: 1000 freq: 4)\n",
      "(term_id:1 doc: 1000 freq: 11)\n",
      "(term_id:2 doc: 1000 freq: 2)\n",
      "(term_id:3 doc: 1000 freq: 5)\n",
      "(term_id:4 doc: 1000 freq: 6)\n",
      "(term_id:5 doc: 1000 freq: 3)\n",
      "(term_id:6 doc: 1000 freq: 4)\n",
      "(term_id:7 doc: 1000 freq: 2)\n",
      "(term_id:8 doc: 1000 freq: 3)\n",
      "(term_id:9 doc: 1000 freq: 2)\n",
      "(term_id:10 doc: 1000 freq: 5)\n",
      "(term_id:11 doc: 1000 freq: 1)\n",
      "(term_id:12 doc: 1000 freq: 7)\n",
      "(term_id:13 doc: 1000 freq: 8)\n",
      "(term_id:14 doc: 1000 freq: 1)\n",
      "(term_id:15 doc: 1000 freq: 4)\n",
      "(term_id:16 doc: 1000 freq: 28)\n",
      "(term_id:17 doc: 1000 freq: 4)\n",
      "(term_id:18 doc: 1000 freq: 16)\n",
      "(term_id:19 doc: 1000 freq: 2)\n",
      "(term_id:20 doc: 1000 freq: 4)\n",
      "(term_id:21 doc: 1000 freq: 1)\n",
      "(term_id:22 doc: 1000 freq: 1)\n",
      "(term_id:23 doc: 1000 freq: 1)\n",
      "(term_id:24 doc: 1000 freq: 13)\n",
      "(term_id:25 doc: 1000 freq: 1)\n",
      "(term_id:26 doc: 1000 freq: 53)\n",
      "(term_id:27 doc: 1000 freq: 9)\n",
      "(term_id:28 doc: 1000 freq: 2)\n",
      "(term_id:29 doc: 1000 freq: 1)\n",
      "(term_id:30 doc: 1000 freq: 18)\n",
      "(term_id:31 doc: 1000 freq: 1)\n",
      "(term_id:32 doc: 1000 freq: 4)\n",
      "(term_id:33 doc: 1000 freq: 11)\n",
      "(term_id:34 doc: 1000 freq: 5)\n",
      "(term_id:35 doc: 1000 freq: 1)\n",
      "(term_id:36 doc: 1000 freq: 4)\n",
      "(term_id:37 doc: 1000 freq: 1)\n",
      "(term_id:38 doc: 1000 freq: 5)\n",
      "(term_id:39 doc: 1000 freq: 1)\n",
      "(term_id:40 doc: 1000 freq: 2)\n",
      "(term_id:41 doc: 1000 freq: 1)\n",
      "(term_id:42 doc: 1000 freq: 1)\n",
      "(term_id:43 doc: 1000 freq: 1)\n",
      "(term_id:44 doc: 1000 freq: 2)\n",
      "(term_id:45 doc: 1000 freq: 2)\n",
      "(term_id:46 doc: 1000 freq: 2)\n",
      "(term_id:47 doc: 1000 freq: 1)\n",
      "(term_id:48 doc: 1000 freq: 1)\n",
      "(term_id:49 doc: 1000 freq: 1)\n",
      "(term_id:50 doc: 1000 freq: 1)\n",
      "(term_id:51 doc: 1000 freq: 2)\n",
      "(term_id:52 doc: 1000 freq: 2)\n",
      "(term_id:53 doc: 1000 freq: 1)\n",
      "(term_id:54 doc: 1000 freq: 2)\n",
      "(term_id:55 doc: 1000 freq: 7)\n",
      "(term_id:56 doc: 1000 freq: 1)\n",
      "(term_id:57 doc: 1000 freq: 3)\n",
      "(term_id:58 doc: 1000 freq: 1)\n",
      "(term_id:59 doc: 1000 freq: 2)\n",
      "(term_id:60 doc: 1000 freq: 1)\n",
      "(term_id:61 doc: 1000 freq: 4)\n",
      "(term_id:62 doc: 1000 freq: 10)\n",
      "(term_id:63 doc: 1000 freq: 1)\n",
      "(term_id:64 doc: 1000 freq: 2)\n",
      "(term_id:65 doc: 1000 freq: 1)\n",
      "(term_id:66 doc: 1000 freq: 4)\n",
      "(term_id:67 doc: 1000 freq: 1)\n",
      "(term_id:68 doc: 1000 freq: 1)\n",
      "(term_id:69 doc: 1000 freq: 2)\n",
      "(term_id:70 doc: 1000 freq: 1)\n",
      "(term_id:71 doc: 1000 freq: 2)\n",
      "(term_id:72 doc: 1000 freq: 2)\n",
      "(term_id:73 doc: 1000 freq: 1)\n",
      "(term_id:74 doc: 1000 freq: 5)\n",
      "(term_id:75 doc: 1000 freq: 13)\n",
      "(term_id:76 doc: 1000 freq: 2)\n",
      "(term_id:77 doc: 1000 freq: 1)\n",
      "(term_id:78 doc: 1000 freq: 1)\n",
      "(term_id:79 doc: 1000 freq: 1)\n",
      "(term_id:80 doc: 1000 freq: 2)\n",
      "(term_id:81 doc: 1000 freq: 1)\n",
      "(term_id:82 doc: 1000 freq: 1)\n",
      "(term_id:83 doc: 1000 freq: 1)\n",
      "(term_id:84 doc: 1000 freq: 4)\n",
      "(term_id:85 doc: 1000 freq: 10)\n",
      "(term_id:86 doc: 1000 freq: 2)\n",
      "(term_id:87 doc: 1000 freq: 4)\n",
      "(term_id:88 doc: 1000 freq: 2)\n",
      "(term_id:89 doc: 1000 freq: 1)\n",
      "(term_id:90 doc: 1000 freq: 2)\n",
      "(term_id:91 doc: 1000 freq: 2)\n",
      "(term_id:92 doc: 1000 freq: 1)\n",
      "(term_id:93 doc: 1000 freq: 2)\n",
      "(term_id:94 doc: 1000 freq: 3)\n",
      "(term_id:95 doc: 1000 freq: 4)\n",
      "(term_id:96 doc: 1000 freq: 2)\n",
      "(term_id:97 doc: 1000 freq: 8)\n",
      "(term_id:98 doc: 1000 freq: 1)\n",
      "(term_id:99 doc: 1000 freq: 4)\n",
      "(term_id:100 doc: 1000 freq: 1)\n",
      "(term_id:101 doc: 1000 freq: 1)\n",
      "(term_id:102 doc: 1000 freq: 2)\n",
      "(term_id:103 doc: 1000 freq: 2)\n",
      "(term_id:104 doc: 1000 freq: 1)\n",
      "(term_id:105 doc: 1000 freq: 1)\n",
      "(term_id:106 doc: 1000 freq: 3)\n",
      "(term_id:107 doc: 1000 freq: 1)\n",
      "(term_id:108 doc: 1000 freq: 1)\n",
      "(term_id:109 doc: 1000 freq: 2)\n",
      "(term_id:110 doc: 1000 freq: 1)\n",
      "(term_id:111 doc: 1000 freq: 5)\n",
      "(term_id:112 doc: 1000 freq: 2)\n",
      "(term_id:113 doc: 1000 freq: 1)\n",
      "(term_id:114 doc: 1000 freq: 2)\n",
      "(term_id:115 doc: 1000 freq: 5)\n",
      "(term_id:116 doc: 1000 freq: 1)\n",
      "(term_id:117 doc: 1000 freq: 1)\n",
      "(term_id:118 doc: 1000 freq: 1)\n",
      "(term_id:119 doc: 1000 freq: 4)\n",
      "(term_id:120 doc: 1000 freq: 1)\n",
      "(term_id:121 doc: 1000 freq: 1)\n",
      "(term_id:122 doc: 1000 freq: 1)\n",
      "(term_id:123 doc: 1000 freq: 1)\n",
      "(term_id:124 doc: 1000 freq: 3)\n",
      "(term_id:125 doc: 1000 freq: 3)\n",
      "(term_id:126 doc: 1000 freq: 4)\n",
      "(term_id:127 doc: 1000 freq: 2)\n",
      "(term_id:128 doc: 1000 freq: 1)\n",
      "(term_id:129 doc: 1000 freq: 1)\n",
      "(term_id:130 doc: 1000 freq: 2)\n",
      "(term_id:131 doc: 1000 freq: 1)\n",
      "(term_id:132 doc: 1000 freq: 1)\n",
      "(term_id:133 doc: 1000 freq: 4)\n",
      "(term_id:134 doc: 1000 freq: 1)\n",
      "(term_id:135 doc: 1000 freq: 1)\n",
      "(term_id:136 doc: 1000 freq: 2)\n",
      "(term_id:137 doc: 1000 freq: 2)\n",
      "(term_id:138 doc: 1000 freq: 1)\n",
      "(term_id:139 doc: 1000 freq: 1)\n",
      "(term_id:140 doc: 1000 freq: 1)\n",
      "(term_id:141 doc: 1000 freq: 2)\n",
      "(term_id:142 doc: 1000 freq: 3)\n",
      "(term_id:143 doc: 1000 freq: 1)\n",
      "(term_id:144 doc: 1000 freq: 1)\n",
      "(term_id:145 doc: 1000 freq: 3)\n",
      "(term_id:146 doc: 1000 freq: 4)\n",
      "(term_id:147 doc: 1000 freq: 1)\n",
      "(term_id:148 doc: 1000 freq: 2)\n",
      "(term_id:149 doc: 1000 freq: 2)\n",
      "(term_id:150 doc: 1000 freq: 1)\n",
      "(term_id:151 doc: 1000 freq: 4)\n",
      "(term_id:152 doc: 1000 freq: 3)\n",
      "(term_id:153 doc: 1000 freq: 2)\n",
      "(term_id:154 doc: 1000 freq: 1)\n",
      "(term_id:155 doc: 1000 freq: 1)\n",
      "(term_id:156 doc: 1000 freq: 1)\n",
      "(term_id:157 doc: 1000 freq: 2)\n",
      "(term_id:158 doc: 1000 freq: 1)\n",
      "(term_id:159 doc: 1000 freq: 3)\n",
      "(term_id:160 doc: 1000 freq: 1)\n",
      "(term_id:161 doc: 1000 freq: 1)\n",
      "(term_id:162 doc: 1000 freq: 1)\n",
      "(term_id:163 doc: 1000 freq: 1)\n",
      "(term_id:164 doc: 1000 freq: 1)\n",
      "(term_id:165 doc: 1000 freq: 1)\n",
      "(term_id:166 doc: 1000 freq: 1)\n",
      "(term_id:167 doc: 1000 freq: 1)\n",
      "(term_id:168 doc: 1000 freq: 3)\n",
      "(term_id:169 doc: 1000 freq: 1)\n",
      "(term_id:170 doc: 1000 freq: 1)\n",
      "(term_id:171 doc: 1000 freq: 1)\n",
      "(term_id:172 doc: 1000 freq: 1)\n",
      "(term_id:173 doc: 1000 freq: 1)\n",
      "(term_id:174 doc: 1000 freq: 1)\n",
      "(term_id:175 doc: 1000 freq: 1)\n",
      "(term_id:176 doc: 1000 freq: 2)\n",
      "(term_id:177 doc: 1000 freq: 1)\n",
      "(term_id:178 doc: 1000 freq: 2)\n",
      "(term_id:179 doc: 1000 freq: 1)\n",
      "(term_id:180 doc: 1000 freq: 1)\n",
      "(term_id:181 doc: 1000 freq: 1)\n",
      "(term_id:182 doc: 1000 freq: 1)\n",
      "(term_id:183 doc: 1000 freq: 2)\n",
      "(term_id:184 doc: 1000 freq: 1)\n",
      "(term_id:185 doc: 1000 freq: 1)\n",
      "(term_id:186 doc: 1000 freq: 1)\n",
      "(term_id:187 doc: 1000 freq: 2)\n",
      "(term_id:188 doc: 1000 freq: 1)\n",
      "(term_id:189 doc: 1000 freq: 1)\n",
      "(term_id:190 doc: 1000 freq: 1)\n",
      "(term_id:191 doc: 1000 freq: 1)\n",
      "(term_id:192 doc: 1000 freq: 2)\n",
      "(term_id:193 doc: 1000 freq: 1)\n",
      "(term_id:194 doc: 1000 freq: 1)\n",
      "(term_id:195 doc: 1000 freq: 1)\n",
      "(term_id:196 doc: 1000 freq: 1)\n",
      "(term_id:197 doc: 1000 freq: 1)\n",
      "(term_id:198 doc: 1000 freq: 1)\n",
      "(term_id:199 doc: 1000 freq: 1)\n",
      "(term_id:200 doc: 1000 freq: 1)\n",
      "(term_id:201 doc: 1000 freq: 1)\n",
      "(term_id:202 doc: 1000 freq: 2)\n",
      "(term_id:203 doc: 1000 freq: 1)\n",
      "(term_id:204 doc: 1000 freq: 1)\n",
      "(term_id:205 doc: 1000 freq: 1)\n",
      "(term_id:206 doc: 1000 freq: 1)\n",
      "(term_id:207 doc: 1000 freq: 1)\n",
      "(term_id:208 doc: 1000 freq: 1)\n",
      "(term_id:209 doc: 1000 freq: 2)\n",
      "(term_id:210 doc: 1000 freq: 2)\n",
      "(term_id:211 doc: 1000 freq: 2)\n",
      "(term_id:212 doc: 1000 freq: 1)\n",
      "(term_id:213 doc: 1000 freq: 1)\n",
      "(term_id:214 doc: 1000 freq: 1)\n",
      "(term_id:215 doc: 1000 freq: 1)\n",
      "(term_id:216 doc: 1000 freq: 1)\n",
      "(term_id:217 doc: 1000 freq: 1)\n",
      "(term_id:218 doc: 1000 freq: 1)\n",
      "(term_id:219 doc: 1000 freq: 1)\n",
      "(term_id:220 doc: 1000 freq: 5)\n",
      "(term_id:221 doc: 1000 freq: 1)\n",
      "(term_id:222 doc: 1000 freq: 1)\n",
      "(term_id:223 doc: 1000 freq: 1)\n",
      "(term_id:224 doc: 1000 freq: 1)\n",
      "(term_id:225 doc: 1000 freq: 1)\n",
      "(term_id:226 doc: 1000 freq: 1)\n",
      "(term_id:227 doc: 1000 freq: 1)\n",
      "(term_id:228 doc: 1000 freq: 1)\n",
      "(term_id:229 doc: 1000 freq: 1)\n",
      "(term_id:230 doc: 1000 freq: 2)\n",
      "(term_id:231 doc: 1000 freq: 1)\n",
      "(term_id:232 doc: 1000 freq: 1)\n",
      "(term_id:233 doc: 1000 freq: 1)\n",
      "(term_id:234 doc: 1000 freq: 1)\n",
      "(term_id:235 doc: 1000 freq: 2)\n",
      "(term_id:236 doc: 1000 freq: 1)\n",
      "(term_id:237 doc: 1000 freq: 1)\n",
      "(term_id:238 doc: 1000 freq: 1)\n",
      "(term_id:239 doc: 1000 freq: 1)\n",
      "(term_id:240 doc: 1000 freq: 1)\n",
      "(term_id:241 doc: 1000 freq: 2)\n",
      "(term_id:242 doc: 1000 freq: 3)\n",
      "(term_id:243 doc: 1000 freq: 1)\n",
      "(term_id:244 doc: 1000 freq: 5)\n",
      "(term_id:245 doc: 1000 freq: 1)\n",
      "(term_id:246 doc: 1000 freq: 1)\n",
      "(term_id:247 doc: 1000 freq: 1)\n",
      "(term_id:248 doc: 1000 freq: 1)\n",
      "(term_id:249 doc: 1000 freq: 1)\n",
      "(term_id:250 doc: 1000 freq: 1)\n",
      "(term_id:251 doc: 1000 freq: 1)\n",
      "(term_id:252 doc: 1000 freq: 1)\n",
      "(term_id:253 doc: 1000 freq: 1)\n",
      "(term_id:254 doc: 1000 freq: 1)\n",
      "(term_id:255 doc: 1000 freq: 1)\n",
      "(term_id:256 doc: 1000 freq: 1)\n",
      "(term_id:257 doc: 1000 freq: 1)\n",
      "(term_id:258 doc: 1000 freq: 1)\n",
      "(term_id:259 doc: 1000 freq: 1)\n",
      "(term_id:260 doc: 1000 freq: 1)\n",
      "(term_id:261 doc: 1000 freq: 1)\n",
      "(term_id:262 doc: 1000 freq: 1)\n",
      "(term_id:263 doc: 1000 freq: 1)\n",
      "(term_id:264 doc: 1000 freq: 1)\n",
      "(term_id:265 doc: 1000 freq: 1)\n",
      "(term_id:266 doc: 1000 freq: 1)\n",
      "(term_id:267 doc: 1000 freq: 1)\n",
      "(term_id:268 doc: 1000 freq: 1)\n",
      "(term_id:269 doc: 1000 freq: 4)\n",
      "(term_id:270 doc: 1000 freq: 1)\n",
      "(term_id:271 doc: 1000 freq: 1)\n",
      "(term_id:272 doc: 1000 freq: 1)\n",
      "(term_id:273 doc: 1000 freq: 1)\n",
      "(term_id:274 doc: 1000 freq: 2)\n",
      "(term_id:275 doc: 1000 freq: 1)\n",
      "(term_id:276 doc: 1000 freq: 1)\n",
      "(term_id:277 doc: 1000 freq: 1)\n",
      "(term_id:278 doc: 1000 freq: 1)\n",
      "(term_id:279 doc: 1000 freq: 1)\n",
      "(term_id:280 doc: 1000 freq: 1)\n",
      "(term_id:281 doc: 1000 freq: 1)\n",
      "(term_id:282 doc: 1000 freq: 1)\n",
      "(term_id:283 doc: 1000 freq: 1)\n",
      "(term_id:284 doc: 1000 freq: 1)\n",
      "(term_id:285 doc: 1000 freq: 1)\n",
      "(term_id:286 doc: 1000 freq: 1)\n",
      "(term_id:287 doc: 1000 freq: 1)\n",
      "(term_id:288 doc: 1000 freq: 1)\n",
      "(term_id:289 doc: 1000 freq: 1)\n",
      "(term_id:290 doc: 1000 freq: 1)\n",
      "(term_id:291 doc: 1000 freq: 1)\n",
      "(term_id:292 doc: 1000 freq: 1)\n",
      "(term_id:293 doc: 1000 freq: 1)\n",
      "(term_id:294 doc: 1000 freq: 1)\n",
      "(term_id:295 doc: 1000 freq: 1)\n",
      "(term_id:296 doc: 1000 freq: 1)\n",
      "(term_id:297 doc: 1000 freq: 1)\n",
      "(term_id:298 doc: 1000 freq: 1)\n",
      "(term_id:299 doc: 1000 freq: 1)\n",
      "(term_id:300 doc: 1000 freq: 1)\n",
      "(term_id:301 doc: 1000 freq: 1)\n",
      "(term_id:302 doc: 1000 freq: 1)\n",
      "(term_id:303 doc: 1000 freq: 14)\n",
      "(term_id:304 doc: 1000 freq: 2)\n",
      "(term_id:305 doc: 1000 freq: 1)\n",
      "(term_id:306 doc: 1000 freq: 1)\n",
      "(term_id:307 doc: 1000 freq: 1)\n",
      "(term_id:308 doc: 1000 freq: 1)\n",
      "(term_id:309 doc: 1000 freq: 1)\n",
      "(term_id:310 doc: 1000 freq: 1)\n",
      "(term_id:311 doc: 1000 freq: 1)\n",
      "(term_id:312 doc: 1000 freq: 1)\n",
      "(term_id:313 doc: 1000 freq: 1)\n",
      "(term_id:314 doc: 1000 freq: 2)\n",
      "(term_id:315 doc: 1000 freq: 1)\n",
      "(term_id:316 doc: 1000 freq: 1)\n"
     ]
    }
   ],
   "source": [
    "#Recuperando o indice\n",
    "from index.indexer import *\n",
    "from index.structure import *\n",
    "import pickle\n",
    "with open(\"occur_index_0.idx\",\"rb\") as idx_file:\n",
    "    obj_index = FileIndex()\n",
    "    t = obj_index.next_from_file(idx_file)\n",
    "    while t != None:\n",
    "        print(t)\n",
    "        t = obj_index.next_from_file(idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
